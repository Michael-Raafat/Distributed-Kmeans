# -*- coding: utf-8 -*-
"""Iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UGiJQLWcOg5URuxE2cPrYymEaKkHYPwg
"""

from sklearn.cluster import KMeans
import pandas as pd 
import numpy as np

data = pd.read_csv("iris.data.txt", header=None) 
values = data.values
data_no_label = values[:,0:4]

kmeans = KMeans(n_clusters=3, random_state=0).fit(data_no_label)
print(kmeans.cluster_centers_)

from sklearn import datasets
from math import log, sqrt
from sklearn.metrics import f1_score as f_measure
def f_measure(ground_truth_labels, clustered_labels, clusters):
	ground_truth_clusters = max(ground_truth_labels)

	tj = [0] * ground_truth_clusters
	for i in ground_truth_labels:
		tj[i - 1] += 1

	maxni = [0] * clusters
	ni = [[0 for i in range(ground_truth_clusters)] for j in range(clusters)]
	for i in range(len(clustered_labels)):
		l = clustered_labels[i]
		ni[l][ground_truth_labels[i] - 1] += 1
		maxni[l] = max(maxni[l], ni[l][ground_truth_labels[i] - 1])

	prec = [maxni[i] / sum(ni[i]) for i in range(clusters)]

	rec = []
	for i in range(clusters):
		mx = 0
		for j in range(ground_truth_clusters):
			mx = max(mx, ni[i][j] / tj[j])
		rec.append(mx)

	f_score = 0
	for i in range(clusters):
		f_score += 2 * prec[i] * rec[i] / (prec[i] + rec[i])
	return f_score / clusters

def conditional_entropy(ground_truth_labels, clustered_labels, clusters):
	n = len(ground_truth_labels)
	ground_truth_clusters = max(ground_truth_labels)

	tj = [0] * ground_truth_clusters
	for i in ground_truth_labels:
		tj[i - 1] += 1

	maxni = [0] * clusters
	ni = [[0 for i in range(ground_truth_clusters)] for j in range(clusters)]
	for i in range(len(clustered_labels)):
		l = clustered_labels[i]
		ni[l][ground_truth_labels[i] - 1] += 1
		maxni[l] = max(maxni[l], ni[l][ground_truth_labels[i] - 1])

	ret = 0
	for i in range(clusters):
		h = 0
		for j in range(ground_truth_clusters):
			p = ni[i][j] / sum(ni[i])
			if p > 0: h -= p * log(p)
		ret += sum(ni[i]) / n * h
	return ret
def readMeans():
	ret = []
	with open('means.TXT', 'r') as f:
		l = f.readlines()

		for line in l:
			point = line.split(' ')
			point = [float(x) for x in point]
			ret.append(point)
	return ret

def dist(a, b):
	ret = 0
	for i in range(len(a)):
		ret += (a[i] - b[i]) ** 2
	return sqrt(ret)

if __name__ == '__main__':
	
	iris = datasets.load_iris()
	ground_truth_labels = iris.target

	means = readMeans()
	clustered_labels = [0] * len(iris.data)

	for i in range(len(iris.data)):
		mnD = dist(means[0], iris.data[i])
		for j in range(1,len(means)):
			cur = dist(means[j], iris.data[i])
			if cur < mnD:
				mnD = cur
				clustered_labels[i] = j

	print('F Measure = ', f_measure(ground_truth_labels, clustered_labels, 3))
	print('Conditional Entropy = ', conditional_entropy(ground_truth_labels, clustered_labels, 3))